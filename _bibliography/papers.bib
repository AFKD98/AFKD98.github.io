---
---

@string{aps = {American Physical Society,}}

@inproceedings{anonymous2025flstore,
title={FLStore: Efficient Federated Learning Storage for non-training workloads},
author={Ahmad Faraz Khan and Samuel Fountain and Ahmed M. Abdelmoniem and Ali R. Butt and Ali Anwar},
abstract = {Federated Learning (FL) is an approach for privacy-preserving Machine Learning (ML), enabling model training across multiple clients without centralized data collection. With an aggregator server coordinating training, aggregating model updates, and storing metadata across rounds. In addition to training, a substantial part of FL systems are the non-training workloads such as scheduling, personalization, clustering, debugging, and incentivization. Most existing systems rely on the aggregator to handle non-training workloads and use cloud services for data storage. This results in high latency and increased costs as non-training workloads rely on large volumes of metadata, including weight parameters from client updates, hyperparameters, and aggregated updates across rounds, making the situation even worse. We propose FLStore, a serverless framework for efficient FL non-training workloads and storage. FLStore unifies the data and compute planes on a serverless cache, enabling locality-aware execution via tailored caching policies to reduce latency and costs. Per our evaluations, compared to cloud object store based aggregator server FLStore reduces per request average latency by 71% and costs by 92.45%, with peak improvements of 99.7% and 98.8%, respectively. Compared to an in-memory cloud cache based aggregator server, FLStore reduces average latency by 64.6% and costs by 98.83%, with peak improvements of 98.8% and 99.6%, respectively. FLStore integrates seamlessly with existing FL frameworks with minimal modifications, while also being fault-tolerant and highly scalable},
booktitle={Eighth Conference on Machine Learning and Systems (MLSys '25)},
year={2025},
url={https://openreview.net/forum?id=OSmHZYq9Ez},
selected = {true}
}

@inproceedings{khan2023pifl,
      title={IP-FL: Incentive-driven Personalization in Federated Learning}, 
      author={Ahmad Faraz Khan and Xinran Wang and Qi Le and Zain ul Abdeen and Azal Ahmad Khan and Haider Ali and Ming Jin and Jie Ding and Ali R. Butt and Ali Anwar},
      year={2025},
      booktitle={the 39th IEEE International Parallel & Distributed Processing Symposium (IPDPS '25)},
      publisher={IEEE},
}

@inproceedings{Khan2024FLOAT,
  author = {Ahmad Faraz Khan and Azal Ahmad Khan and Ahmed M. Abdelmoniem and Samuel Fountain and Ali R. Butt and Ali Anwar},
  title = {{FLOAT}: Federated Learning Optimizations with Automated Tuning},
  booktitle = {Nineteenth European Conference on Computer Systems (EuroSys '24)},
  year = {2024},
  date = {April 22--25},
  location = {Athens, Greece},
  pages = {19},
  abstract = {Federated Learning (FL) has emerged as a powerful approach that enables collaborative distributed model training without the need for data sharing. However, FL grapples with inherent heterogeneity challenges leading to issues such as stragglers, dropouts, and performance variations. Selection of clients to run an FL instance is crucial, but existing strategies introduce biases and participation issues and do not consider resource efficiency. Communication and training acceleration solutions proposed to increase client participation also fall short due to the dynamic nature of system resources. We address these challenges in this paper by designing FLOAT, a novel framework designed to boost FL client resource awareness. FLOAT optimizes resource utilization dynamically for meeting training deadlines, and mitigates stragglers and dropouts through various optimization techniques; leading to enhanced model convergence and improved performance. FLOAT leverages multi-objective Reinforcement Learning with Human Feedback (RLHF) to automate the selection of the optimization techniques and their configurations, tailoring them to individual client resource conditions. Moreover, FLOAT seamlessly integrates into existing FL systems, maintaining non-intrusiveness and versatility for both asynchronous and synchronous FL settings. As per our evaluations, FLOAT increases accuracy by up to 53%, reduces client dropouts by up to 78×, and improves communication, computation, and memory utilization by up to 81×, 44×, and 20× respectively.},
  publisher = {ACM},
  address = {New York, NY, USA},
  doi = {10.1145/3627703.3650081},
  url = {https://doi.org/10.1145/3627703.3650081},
  selected = {true}
}

@inproceedings {wang2024incentivized,
author = { Wang, Xinran and Le, Qi and Khan, Ahmad Faraz and Ding, Jie and Anwar, Ali },
booktitle = { 2024 IEEE International Conference on Big Data (BigData) },
title = {{ ICL: An Incentivized Collaborative Learning Framework }},
year = {2024},
volume = {},
ISSN = {},
pages = {94-103},
abstract = { Collaborations among various entities, such as companies, research labs, AI agents, and edge devices, have become increasingly crucial for achieving machine learning tasks that cannot be accomplished by a single entity alone. This is likely due to factors such as security constraints, privacy concerns, and limitations in computation resources. As a result, Collaborative Learning has been gaining momentum. However, a significant challenge in practical applications of Collaborative Learning is how to effectively incentivize multiple entities to collaborate before any collaboration occurs. In this study, we propose ICL, an architectural framework for Incentivized Collaborative Learning, and provide insights into the critical issue of when and why incentives can improve collaboration performance. We showcase the concepts of ICL to specific use cases in federated learning, assisted learning, and multi-armed bandit, corroborating with both theoretical and experimental results. },
keywords = {Privacy;Data privacy;Federated learning;Collaboration;Pricing;Companies;Big Data;Data models;Security},
doi = {10.1109/BigData62323.2024.10825643},
url = {https://doi.ieeecomputersociety.org/10.1109/BigData62323.2024.10825643},
publisher = {IEEE Computer Society},
address = {Los Alamitos, CA, USA},
month =Dec}

@inproceedings {pFL_techniques,
author = { Khan, Azal Ahmad and Khan, Ahmad Faraz and Ali, Haidar and Anwar, Ali },
booktitle = { 2024 IEEE International Conference on Big Data (BigData) },
title = {{ Personalized Federated Learning Techniques: Empirical Analysis }},
year = {2024},
volume = {},
ISSN = {},
pages = {1333-1339},
abstract = { Personalized Federated Learning (pFL) holds immense promise for tailoring machine learning models to individual users while preserving data privacy. However, achieving optimal performance in pFL often requires a careful balancing act between memory overhead costs and model accuracy. This paper delves into the trade-offs inherent in pFL, offering valuable insights for selecting the right algorithms for diverse real-world scenarios. We empirically evaluate ten prominent pFL techniques across various datasets and data splits, uncovering significant differences in their performance. Our study reveals interesting insights into how pFL methods that utilize personalized (local) aggregation exhibit the fastest convergence due to their efficiency in communication and computation. Conversely, fine-tuning methods face limitations in handling data heterogeneity and potential adversarial attacks while multi-objective learning methods achieve higher accuracy at the cost of additional training and resource consumption. Our study emphasizes the critical role of communication efficiency in scaling pFL, demonstrating how it can significantly affect resource usage in real-world deployments. },
keywords = {Training;Measurement;Costs;Accuracy;Machine learning algorithms;Federated learning;Memory management;Medical services;Market research;Data models},
doi = {10.1109/BigData62323.2024.10825575},
url = {https://doi.ieeecomputersociety.org/10.1109/BigData62323.2024.10825575},
publisher = {IEEE Computer Society},
address = {Los Alamitos, CA, USA},
month =Dec}

@inproceedings {LLM_sycophancy,
author = { Khan, Azal Ahmad and Alam, Sayan and Wang, Xinran and Khan, Ahmad Faraz and Neog, Debanga Raj and Anwar, Ali },
booktitle = { 2024 IEEE International Conference on Big Data (BigData) },
title = {{ Mitigating Sycophancy in Large Language Models via Direct Preference Optimization }},
year = {2024},
volume = {},
ISSN = {},
pages = {1664-1671},
abstract = { Large language models (LLMs) have demonstrated remarkable capabilities, yet they occasionally exhibit sycophantic behavior, generating responses that align with or agree with a user’s stated opinions or preferences, even when those opinions are incorrect or biased. This sycophantic tendency can undermine the trustworthiness and reliability of LLMs. This work proposes a novel approach to mitigate sycophancy in LLMs by fine-tuning them on a carefully curated dataset comprising prompts paired with sycophantic and non-sycophantic responses 1. Our method leverages Direct Preference Optimization (DPO), which optimizes LLMs to generate responses that align with the preferred (non-sycophantic) outputs without requiring explicit reward modeling. We develop a dataset of 1000 prompts with sycophantic and non-sycophantic responses to fine-tune LLMs. Our approach achieves an average reduction of 85% in persona-based tests and 84% in preference-driven tests, demonstrating significant mitigation of sycophantic behaviors. Our findings pave the way for more trustworthy and reliable language models that can provide objective and unbiased responses, aligning with human preferences while maintaining factual accuracy. },
keywords = {Ethics;Accuracy;Large language models;Prevention and mitigation;Big Data;Data models;Reliability;Optimization},
doi = {10.1109/BigData62323.2024.10825538},
url = {https://doi.ieeecomputersociety.org/10.1109/BigData62323.2024.10825538},
publisher = {IEEE Computer Society},
address = {Los Alamitos, CA, USA},
month =Dec,
selected = {true}}

@inproceedings {dynamicFL,
author = { Le, Qi and Diao, Enmao and Wang, Xinran and Khan, Ahmad Faraz and Tarokh, Vahid and Ding, Jie and Anwar, Ali },
booktitle = {2024 IEEE International Conference on Big Data (BigData) *Awarded Best Paper*},
title = {{ DynamicFL: Federated Learning with Dynamic Communication Resource Allocation }},
year = {2024},
volume = {},
ISSN = {},
pages = {998-1008},
abstract = { Federated Learning (FL) is a collaborative machine learning framework that allows multiple users to train models utilizing their local data in a distributed manner. However, considerable statistical heterogeneity in local data across devices often leads to suboptimal model performance compared with independently and identically distributed (IID) data scenarios. In this paper, we introduce DynamicFL, a new FL framework that investigates the trade-offs between global model performance and communication costs for two widely adopted FL methods: Federated Stochastic Gradient Descent (FedSGD) and Federated Averaging (FedAvg). Our approach allocates diverse communication resources to clients based on their data statistical heterogeneity, considering communication resource constraints, and attains substantial performance enhancements compared to uniform communication resource allocation. Notably, our method bridges the gap between FedSGD and FedAvg, providing a flexible framework leveraging communication heterogeneity to address statistical heterogeneity in FL. Through extensive experiments, we demonstrate that DynamicFL surpasses current state-of-the-art methods with up to a 10% increase in model accuracy, demonstrating its adaptability and effectiveness in tackling data statistical heterogeneity challenges. },
keywords = {Performance evaluation;Adaptation models;Costs;Accuracy;Federated learning;Distributed databases;Stochastic processes;Data models;Robustness;Resource management},
doi = {10.1109/BigData62323.2024.10826074},
url = {https://doi.ieeecomputersociety.org/10.1109/BigData62323.2024.10826074},
publisher = {IEEE Computer Society},
address = {Los Alamitos, CA, USA},
month =Dec,
selected = {true}}





@article {Emon2024.10.30.621203,
	author = {Emon, Muhit Islam and Cheung, Yat Fei and Stoll, James and Rumi, Monjura Afrin and Brown, Connor and Choi, Joung Min and Moumi, Nazifa Ahmed and Ahmed, Shafayat and Song, Haoqiu and Sein, Justin and Yao, Shunyu and Khan, Ahmad and Gupta, Suraj and Kulkarni, Rutwik and Butt, Ali and Vikesland, Peter and Pruden, Amy and Zhang, Liqing},
	title = {CIWARS: a web server for waterborne antibiotic resistance surveillance using longitudinal metagenomic data},
	elocation-id = {2024.10.30.621203},
	year = {2024},
	doi = {10.1101/2024.10.30.621203},
	publisher = {Cold Spring Harbor Laboratory},
	abstract = {The rise of antibiotic resistance (AR) is a major global health crisis, exacerbated by the overuse and misuse of antibiotics, leading to the rapid spread of antibiotic resistance genes (ARGs) in bacterial pathogens. This phenomenon poses significant threats to human and animal health, food security, and economic stability. Water bodies, particularly wastewater treatment plants (WWTPs), serve as critical reservoirs for ARGs, creating environments that favor the proliferation of resistant bacteria. Wastewater-based surveillance (WBS) has emerged as a cost-effective strategy for monitoring AR at the population level, providing real-time data to guide public health and policy decisions. Despite advancements in WBS, there are no comprehensive online analytical platforms for continuous environmental AR surveillance. This paper introduces CIWARS, a web server designed for AR analyses of longitudinal metagenomic data. CIWARS offers comprehensive ARG profiling, taxonomic annotation, and anomalous AR risk points detection. We demonstrate its capabilities through an interactive temporal data visualization, showcasing its potential for enhancing AR risk monitoring and guiding effective mitigation strategies. CIWARS is broadly applicable to longitudinal metagenomic data generated from any environment and aims to support global efforts in addressing the AR crisis by providing cyberinfrastructure for continuous AR surveillance. The web server is freely available at https://ciwars.cs.vt.edu/.Competing Interest StatementThe authors have declared no competing interest.},
	URL = {https://www.biorxiv.org/content/early/2024/11/03/2024.10.30.621203},
	eprint = {https://www.biorxiv.org/content/early/2024/11/03/2024.10.30.621203.full.pdf},
	journal = {bioRxiv}
}


@inproceedings{adaptiveAggregationKhan,
  author={Khan, Ahmad Faraz and Li, Yuze and Wang, Xinran and Haroon, Sabaat and Ali, Haider and Cheng, Yue and Butt, Ali R. and Anwar, Ali},
  booktitle={2023 IEEE International Conference on Big Data (BigData)}, 
  title={Towards cost-effective and resource-aware aggregation at Edge for Federated Learning}, 
  year={2023},
  volume={},
  number={},
  pages={690-699},
  keywords={Data centers;Data privacy;Costs;Federated learning;Scalability;Big Data;Data transfer;federated learning;aggregation;edge computing},
  doi={10.1109/BigData59044.2023.10386691}
}

@ARTICLE{securitySurvey,
  author={Ali, Haider and Chen, Dian and Harrington, Matthew and Salazar, Nathaniel and Ameedi, Mohannad Al and Khan, Ahmad Faraz and Butt, Ali R. and Cho, Jin-Hee},
  journal={IEEE Access}, 
  title={A Survey on Attacks and Their Countermeasures in Deep Learning: Applications in Deep Neural Networks, Federated, Transfer, and Deep Reinforcement Learning}, 
  year={2023},
  volume={11},
  number={},
  pages={120095-120130},
  keywords={Surveys;Deep learning;Measurement;Artificial neural networks;Transfer learning;Market research;Security;Federated learning;Reinforcement learning;Attacks;defenses;deep neural networks;federated learning;transfer learning;deep reinforcement learning},
  doi={10.1109/ACCESS.2023.3326410}
  }


@INPROCEEDINGS{TIFF,
  author={Han, Jingoo and Khan, Ahmad Faraz and Zawad, Syed and Anwar, Ali and Angel, Nathalie Baracaldo and Zhou, Yi and Yan, Feng and Butt, Ali R.},
  booktitle={2022 IEEE 15th International Conference on Cloud Computing (CLOUD)}, 
  title={TIFF: Tokenized Incentive for Federated Learning}, 
  year={2022},
  volume={},
  number={},
  pages={407-416},
  keywords={Training;Cloud computing;Atmospheric measurements;Data integrity;Particle measurements;Collaborative work;Loss measurement;Federated learning;Privacy-aware machine learning;Incentive mechanism;Tokenized incentivization;Distributed deep learning},
  doi={10.1109/CLOUD55607.2022.00064}
  }

@INPROCEEDINGS{heterogenityJingoo,
  author={Han, Jingoo and Khan, Ahmad Faraz and Zawad, Syed and Anwar, Ali and Angel, Nathalie Baracaldo and Zhou, Yi and Yan, Feng and Butt, Ali R.},
  booktitle={2022 IEEE International Conference on Big Data (Big Data)}, 
  title={Heterogeneity-Aware Adaptive Federated Learning Scheduling}, 
  year={2022},
  volume={},
  number={},
  pages={911-920},
  keywords={Performance evaluation;Training;Adaptive scheduling;Adaptation models;Adaptive systems;Federated learning;Big Data;Federated learning;Privacy-aware and secure deep learning;Federated learning scheduling;Resource management and scheduling;Distributed deep learning},
  doi={10.1109/BigData55660.2022.10020721}}
