{
  "basics": {
    "name": "Ahmad Faraz Khan",
    "label": "Ph.D. Candidate in Computer Science",
    "email": "ahmadfk@vt.edu",
    "url": "https://afkd98.github.io",
    "summary": "Ph.D. candidate specializing in Machine Learning Systems, with a robust focus on Federated Learning optimization. Demonstrates comprehensive expertise across programming languages and development tools, contributing significantly to both academic research and practical applications.",
    "location": {
      "city": "Blacksburg",
      "region": "VA",
      "countryCode": "US"
    },
    "profiles": [
      {
        "network": "LinkedIn",
        "username": "ahmadfarazkhandurrani",
        "url": "https://www.linkedin.com/in/ahmadfarazkhandurrani/"
      }
    ]
  },
  "education": [
    {
      "institution": "Virginia Tech",
      "area": "Computer Science",
      "studyType": "Ph.D.",
      "startDate": "2020-12",
      "endDate": "Present",
      "gpa": "3.8",
      "courses": [
        "Machine Learning Systems",
        "Distributed Systems",
        "Deep Learning",
        "Machine Learning",
        "Cloud Development",
        "Computer Systems"
      ]
    },
    {
      "institution": "LUMS",
      "area": "Computer Science",
      "studyType": "B.S.",
      "startDate": "2016-01",
      "endDate": "2020-01",
      "courses": [
        "Distributed Systems",
        "Deep Learning",
        "Machine Learning",
        "Cloud Development",
        "Computer Systems"
      ]
    }
  ],
  "work": [
    {
      "name": "Virginia Tech, DSSL",
      "position": "Graduate Research Assistant",
      "startDate": "2020-12",
      "endDate": "Present",
      "summary": "Mentored by Dr. Ali Butt, focused on developing solutions for resource-constrained learning. My research spans the design of distributed systems, enhanced learning schedulers, and the fine-tuning of Large Language Models (LLMs), aiming at optimizing resource utilization, accuracy, and efficiency in privacy-aware learning environments.",
      "highlights": [
        "Built a distributed learning system in Pytorch for resource-constrained privacy-aware learning, enhancing resource utilization by 81x, scalability by 78x, and accuracy by 53%.",
        "Designed a distributed learning parameter server on Hadoop Spark to support over one million learning nodes, increasing scalability by 4x, reducing latency by 8x, and cutting costs by 2x.",
        "Developed a scheduler for distributed learning systems in Pytorch, which improved accuracy by 57% and reduced training time by 40%.",
        "Engineered an efficient, infinitely scalable, and cost-effective cache on AWS Lambda, ElastiCache, SageMaker, and EC2 for non-training workloads, decreasing latency by 99.9% and costs by 99.6%.",
        "Improved distributed ML schedulers in Pytorch to identify and eliminate adversarial data sources, increasing accuracy by 7% by successfully mitigating 100% of malicious data sources.",
        "Developed clustering-based personalized learning solutions in Pytorch for distributed ML systems, enhancing personalized accuracy by up to 45%.",
        "Designed a RAG-based context-aware LLM framework using Hugging Face and Pytorch, automating the adaptive online configuration of distributed cloud services to enhance resource efficiency.",
        "Implemented a Direct Preference Optimization (DPO)-based approach to mitigate sycophancy by fine-tuning LLMs on our curated dataset, reducing sycophancy by 64% in persona-based tests and 44% in preference-driven tests.",
        "Developed a DPO-based approach for prompt optimization without separate reward modeling for LLMs, improving score by 27% compared to supervised fine-tuning."
      ]
    }
    
    
    
    ,
    {
      "name": "i2c Inc.",
      "position": "Associate Data Engineer",
      "startDate": "2020-05",
      "endDate": "2020-12",
      "summary": "Spearheaded the development and maintenance of distributed databases, focusing on performance optimization and scalability."
    }
  ],
  "publications": [
    {
      "name": "FLStore: A Cache for Non-Training Workloads in Federated Learning",
      "publisher": "Submitted: USENIX FAST'25",
      "releaseDate": "2025-7-31",
      "summary": "Designed 'FLStore' locality-aware processing cache for handling non-training workloads of distributed privacy-aware learning efficiently at low cost. Decreased latency up to 99.9% and cost up to 99.6%"
    },    
    {
      "name": "DynamicFL: Federated Learning with Dynamic Communication Resource Allocation",
      "publisher": "Submitted: ACM EuroSys'25",
      "releaseDate": "2025-7-31",
      "summary": "Designed 'DynamicFL' that dynamically allocates communication resources in distributed learning based on data heterogeneity, enhancing model accuracy by up to 10% compared to standard methods."
    },
    {
      "name": "FLOAT: Federataed Learning Optimizations with Automated Tuning",
      "publisher": "Published: ACM EuroSys'24",
      "releaseDate": "2024-4-22",
      "summary": "Designed 'FLOAT', a framework for enabling distributed learning and fine-tuning with high efficiency and resource utilization at low cost on constrained and heterogeneous Edge devices, leveraging Reinforcement Learning with Human Feedback. Improved resource utilization by 81x, scalability by 78x, and accuracy by 53%."
    },
    {
      "name": "Enhancing Personalized Distributed Learning with Incentives",
      "publisher": "Submitted: NeurIPS'24",
      "releaseDate": "2024-4-15",
      "summary": "Designed an incentive-driven personalized training and fine-tuning distributed learning framework for resource-constrained data-heterogeneous Edge devices. Enhanced personalized accuracy by up to 45%."
    },
    {
      "name": "Analyzing Personalized Machine Learning Algorithms",
      "publisher": "Submitted: VLDB'24",
      "releaseDate": "2024-7-9",
      "summary": "Conducted a thorough analysis of personalized ML algorithms, highlighting the trade-offs between privacy and performance."
    },
    {
      "name": "Mitigating sycophancy in LLMs",
      "publisher": "Submitted: ICML Workshop'24",
      "releaseDate": "2024-7-9",
      "summary": "Introduced a Direct Preference Optimization approach to mitigate sycophancy by fine-tuning LLMs on a curated dataset. Reduced sycophancy by 64% in persona-based tests and 44% in preference-driven tests."
    },
    {
      "name": "Prompt optimization for LLMs",
      "publisher": "Submitted: COLM'24",
      "releaseDate": "2024-7-9",
      "summary": "Developed a Direct Preference Optimization approach harnessing human preferences for prompt optimization of text-to-image tasks. Improved score by 27% compared to supervised fine-tuning"
    },
    {
      "name": "Adaptive Machine Learning Aggregator for Edge and IoT",
      "publisher": "Published: IEEE BigData'23",
      "releaseDate": "2023-12-15",
      "summary": "Developed an adaptive aggregator that significantly enhances scalability and time efficiency for Federated Learning on Edge and IoT devices. Increased scalability by 4x, reducing latency by 8x, and cutting costs by 2x"
    },
    {
      "name": "Survey on Adversarial Tactics in DNN, DRL, FL, and TL Models",
      "publisher": "Published: IEEE Access'24",
      "releaseDate": "2023-10-20",
      "summary": "Conducted a survey on adversarial tactics in deep learning models, emphasizing their applications and distinct features."
    },
    {
      "name": "Distributed Learning Schedulers",
      "publisher": "Published: FL-AAAI’22, IEEE CLOUD’22, AAAI-AIES’24",
      "releaseDate": "2022-7-10",
      "summary": "Developed a scheduler for distributed learning systems in Pytorch, which improved accuracy by 57% and reduced training time by 40%."
    },
    {
      "name": "Heterogeneity-Aware Adaptive Machine Learning Scheduling System",
      "publisher": "Published: IEEE BigData’22",
      "releaseDate": "2022-12-17",
      "summary": "Designed a heterogeneity-aware scheduler for distributed learning systems in Pytorch to manage efficiency and accuracy. Improved accuracy by 57% and training time by 40%."
    },
    {
      "name": "Privacy Preserving and Feature Importance Based Incentive Mechanism in Vertical Federated Learning",
      "publisher": "In Progress",
      "releaseDate": "2022-7-9",
      "summary": "Monetized VFL with PERFACY-FL, an incentive mechanism valuing data quality and privacy using Homomorphic Encryption, boosting participation and profitability."
    },
    {
      "name": "PETER: Privacy-Preserving Vertical Federated Learning Against Feature Inference Attacks",
      "publisher": "Submitted: TIFS'24",
      "releaseDate": "2025-6-12",
      "summary": "Proposed a lossless and efficient defense mechanism for inference attacks in Vertical Federated Learning environments."
    }
  ],
  "skills": [
    {
      "name": "Programming Languages",
      "level": "Expert",
      "keywords": [
        "Python",
        "Javascript",
        "C/C++",
        "Java",
        "Go"
      ]
    },
    {
      "name": "Tools & Libraries",
      "level": "Expert",
      "keywords": [
        "PySpark",
        "AWS Suite",
        "Pandas",
        "Numba",
        "Dask",
        "Docker",
        "PyTorch",
        "TensorFlow",
        "IBMfl lib",
        "FedScale",
        "Selenium",
        "Appium",
        "gnuplot",
        "ES6+",
        "TypeScript",
        "React/Redux",
        "Node.js",
        "Express",
        "MongoDB",
        "SQL",
        "FLSim"
      ]
    }
  ],
  "languages": [
    {
      "language": "English",
      "fluency": "Fluent"
    }
  ],
  "interests": [
    {
      "name": "Federated Learning",
      "keywords": [
        "Resource Optimization",
        "Model Performance",
        "System and Data Heterogeneity"
      ]
    },
    {
      "name": "Machine Learning Systems",
      "keywords": [
        "Scalability",
        "Efficiency",
        "Cloud Development"
      ]
    }
  ],
  "projects": [
    {
      "name": "Federated Learning Frameworks",
      "summary": "Led the design and development of both Horizontal and Vertical Federated Learning frameworks, integrating MLOps pipelines with AWS cloud resources.",
      "highlights": [
        "HFL & VFL framework development",
        "MLOps pipeline integration with AWS",
        "Significant contributions to AAAI'24 and AAMAS'24"
      ],
      "startDate": "2020-12",
      "endDate": "Present"
    },
    {
      "name": "ML System Optimization",
      "summary": "Developed algorithms to enhance ML system architectures for improved resource allocation, scalability, and efficiency.",
      "highlights": [
        "Algorithm development for system optimization",
        "Impactful contributions leading to publications in IEEE and ACM conferences"
      ],
      "startDate": "2020-12",
      "endDate": "Present"
    }
  ]
}
